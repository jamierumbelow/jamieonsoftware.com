<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://fonts.googleapis.com/css?family=Bebas+Neue&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <title>ai &centerdot; Jamie on Software</title>
  </head>
  <body>
    <header class="callout">
      <h1>
        <a href="https://jamieonsoftware.com/">
          Jamie on Software
        </a>
      </h1>
      <p></p>
    </header>

    <div class="content homepage-content wrap">
      <section class="posts">
        <article class="post text">
          <p>This is the blog of <a href="http://jamierumbelow.net">Jamie Rumbelow</a>: a software engineer, writer and philosophy graduate who lives in London.</p>
          <p>
  <strong>tags:</strong>
    
  <a href="/aesthetics">aesthetics</a>
    
  <strong>ai</strong>
    
  <a href="/books">books</a>
    
  <a href="/climate">climate</a>
    
  <a href="/decentralisation">decentralisation</a>
    
  <a href="/economics">economics</a>
    
  <a href="/effective-altruism">effective-altruism</a>
    
  <a href="/ethereum">ethereum</a>
    
  <a href="/fintech">fintech</a>
    
  <a href="/food">food</a>
    
  <a href="/housing">housing</a>
    
  <a href="/javascript">javascript</a>
    
  <a href="/links">links</a>
    
  <a href="/london">london</a>
    
  <a href="/music">music</a>
    
  <a href="/personal">personal</a>
    
  <a href="/philosophy">philosophy</a>
    
  <a href="/politics">politics</a>
    
  <a href="/productivity">productivity</a>
    
  <a href="/progress">progress</a>
    
  <a href="/science">science</a>
    
  <a href="/space">space</a>
    
  <a href="/startups">startups</a>
    
  <a href="/statistics">statistics</a>
    
  <a href="/urbanism">urbanism</a>
   
</p>

          <p>
  <strong>months:</strong>
       
  <a href="/months/2023/01">
    January 2023
  </a>
        
  <a href="/months/2022/12">
    December 2022
  </a>
        
  <a href="/months/2022/11">
    November 2022
  </a>
        
  <a href="/months/2022/10">
    October 2022
  </a>
            
  <a href="/months/2022/09">
    September 2022
  </a>
        
  <a href="/months/2022/08">
    August 2022
  </a>
            
  <a href="/months/2022/07">
    July 2022
  </a>
        
  <a href="/months/2022/06">
    June 2022
  </a>
            
  <a href="/months/2022/05">
    May 2022
  </a>
            
  <a href="/months/2022/04">
    April 2022
  </a>
                    
  <a href="/months/2022/03">
    March 2022
  </a>
            
  <a href="/months/2022/01">
    January 2022
  </a>
            
  <a href="/months/2021/05">
    May 2021
  </a>
        
  <a href="/months/2020/02">
    February 2020
  </a>
            
  <a href="/months/2020/01">
    January 2020
  </a>
        
  <a href="/months/2019/12">
    December 2019
  </a>
        
</p>

          <hr />
        </article>

        
          <article class="post text">
            <h2 class="post-title"><a href="https://jamieonsoftware.com/2023/01/09/links-december-2022.html">Links, December 2022</a></h2>
            <div class="post-content">
              <p>December passed by in a blur, with our wedding, my birthday, a lot of travel, and more champagne than I care to mention. I had plenty of time to read and relax, and have found myself in January with a newfound sense of focus and purpose. I am a part of something bigger now. The boundaries of my responsibilities extend beyond myself. I now owe myself, my ambitions, my successes, fully to another person. A conspiracy of two.</p>

<p><a href="https://www.amazon.co.uk/Project-Hail-Mary-Andy-Weir/dp/0593135202">Project Hail Mary</a> was fun, although I think it tried to be too smart – the science was a little too complicated in a way that felt unbelievable – which made it a poorer book than the Martian. <a href="https://www.amazon.co.uk/Man-Who-Solved-Market-Revolution/dp/073521798X">The Man Who Solved The Market</a> was just interesting enough to get me through a four-hour flight.</p>

<p><a href="https://www.centreforcities.org/publication/a-very-short-guide-to-planning-reform/">A very short guide to planning reform</a>, a useful resource to lean on when YIMBY-proselytising. Addresses a lot of the common misconceptions. And <a href="https://stephenhoskins.notion.site/Pennington-K-2021-Does-Building-New-Housing-Cause-Displacement-The-Supply-and-Demand-Effects--505dfd143406488d956eaa59e2435dbb">gentrification without displacement</a>.</p>

<p>Jatan curates some <a href="https://blog.jatan.space/p/lunasights">photos of the Moon</a>. The <a href="https://www.amazon.co.uk/Apollo-Remastered-Andy-Saunders/dp/024150869X">Apollo Remastered book is astounding</a>. And Andrew McCarthy gives us the <a href="https://twitter.com/AJamesMcCarthy/status/1600749501664612353">following gift</a>:</p>

<p><img src="https://pbs.twimg.com/media/Fjb9bbBUAAAAgoH?format=jpg&amp;name=4096x4096" alt="Marsrise" /></p>

<p>ChatGPT can <a href="https://maximumeffort.substack.com/p/i-taught-chatgpt-to-invent-a-language">invent a fairly consistent, heavily inflected language</a>. A fun example use, but I’ll be more impressed when it develops some irregular verbs from some sort of environmental or phonetic pressure.</p>

<p><a href="https://www.getguesstimate.com">Guesstimate</a> is a really good little tool for generating Fermi estimates.</p>

<p>Science has been doing an experiment on itself for the past 60 years. That experiment is called ‘peer review’, and <a href="https://experimentalhistory.substack.com/p/the-rise-and-fall-of-peer-review">it failed</a>.</p>

<p>Rohit performs <a href="https://www.strangeloopcanon.com/p/agi-strange-equation">a Fermi estimate for scary AI</a>.</p>

<p>Gastro Obscura <a href="https://www.atlasobscura.com/articles/perpetual-broth">on pot-au-feu</a>.</p>

<p>Gavin Leech’s <a href="https://twitter.com/g_leech_/status/1607313892644122626">papers of the year list</a>.</p>

<p><a href="https://twitter.com/LlamaInaTux/status/1607392231119360007">2022’s Twitter trends</a>. A brilliant reminder of how ephemeral most of the discourse is.</p>

<p>Lowell <a href="https://twitter.com/lucafrighetti/status/1607758734306193409">projects his own eyeball onto Mars and Venus</a>.</p>

<p>Simon Sarris <a href="https://simonsarris.substack.com/p/how-to-cook-for-odysseus">develops a diet I can relate to</a>.</p>

<p>Nabeel argues for <a href="https://twitter.com/nabeelqu/status/1608874209375313920">reading slowly and deeply</a>.</p>

            </div>
            <footer class="post-meta">
              <span> 1:36pm. January  9, 2023.</span>
              
            </footer>
          </article>
          <hr />
        
          <article class="post text">
            <h2 class="post-title"><a href="https://jamieonsoftware.com/2022/12/01/links-november-2022.html">Links, November 2022</a></h2>
            <div class="post-content">
              <p>Finished the first draft of my book, <a href="https://www.amazon.co.uk/Building-Ethereum-Products-Protocols-Platforms/dp/1484290445">Building With Ethereum: Products, Protocols, and Platforms</a>, which is now available to pre-order from Amazon. Some minor edits to do and then to production. Other than that, I’ve been enjoying drinking again – more on that soon – and reading, relaxing, resting. I’m taking the rest of the year off to plan and scheme.</p>

<p>Heavy fiction month. The Rest is History <a href="https://play.acast.com/s/the-rest-is-history-podcast/185-agatha-christie">prompted me</a> to start reading some Agatha Cristie, so I sat down over the weekend and read <a href="https://www.amazon.co.uk/Then-There-Were-None-Favourite/dp/0008123209">And Then There Were None</a>, <a href="https://www.amazon.co.uk/Murder-Roger-Ackroyd-Poirot/dp/0007527527">The Murder of Roger Ackroyd</a>, and <a href="https://www.amazon.co.uk/Sleeping-Murder-Marple-Agatha-Christie/dp/000819663X">Sleeping Murder</a>. The writing is charming and lovely, and funny, even when grim. You truly get the sense that she’s building these tropes as she goes along, that the structure of the whodunnit genre was falling out of her pen as she went. Totally gripping. I’ve not enjoyed fiction this effortlessly since Station Eleven.</p>

<p>I also read <a href="https://www.amazon.co.uk/Red-Mars-Kim-Stanley-Robinson/dp/0007310161">Red Mars</a> by Kim Stanley Robinson, a good novel but about 200 pages longer than it needed to be. I find the same problem with most scifi and fantasy. Something about world-building lends itself to self-indulgence? I don’t know.</p>

<p>Reread David Yaffe’s <a href="https://www.amazon.co.uk/Reckless-Daughter-Portrait-Joni-Mitchell/dp/0374538069">Reckless Daughter</a> in preperation for my Joni Mitchell Interintellect salon, which went well. I put that book down liking her much less as a person and much more as an artist.</p>

<p><a href="https://www.explainpaper.com">Upload a paper, highlight confusing text, get an explanation.</a></p>

<p>An interesting approach to price iteration and programmable pricing structures: <a href="https://priceops.org">theory</a> and <a href="https://www.tier.run">practice</a>.</p>

<p>Some good housing stuff this month too. Much more energy toward this issue in my little corner of the internet. <a href="https://twitter.com/MrSteveDriscoll/status/1587249795428589568">Auckland upzoning reforms working well</a>, causing <a href="https://onefinaleffort.com/auckland">a meaningful decrease in real rents and an increase in affordability</a>.</p>

<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4266459">Public misunderstanding of housing and measuring the impact of ‘supply skepticism’</a>; YIMBY campagining is evermore important.</p>

<p><a href="https://forum.effectivealtruism.org/posts/6NnnPvzCzxWpWzAb8/podcast-the-left-and-effective-altruism-with-habiba-islam">Good interview with Habiba Islam on the Left and Effective Altruism.</a> I’d like to see something similar for the Scrutonian right.</p>

<p><a href="https://rootsofprogress.org/devanney-on-the-nuclear-flop">Why has nuclear power been a flop?</a></p>

<p><a href="https://twitter.com/DanNeidle/status/1589657806315323395">How much goes in tax, and how much goes to the winemaker, when you buy a bottle of wine?</a></p>

<p><a href="https://www.youtube.com/watch?app=desktop&amp;v=73Dx3Ts1ze4">Good introduction to LLMs.</a></p>

<p><a href="http://steveharoz.com/blog/2022/reviewing-tip-the-10-minute-data-check/">An excellent set of simple statistical checks</a> to apply to analyses in academic papers.</p>

<p><a href="https://www.ons.gov.uk/census/maps">The new UK census maps</a> are great.</p>

<p>The excellent Ethan Mollick <a href="https://oneusefulthing.substack.com/p/when-survivorship-bias-meets-superstitious">on survivorship bias</a>,</p>

<p>Jamie on Software is a <a href="https://jamieonsoftware.com/2021/05/16/books-i-wish-existed.html">big</a> <a href="http://jamieonsoftware.com/books/">fan</a> of <a href="https://jamierumbelow.net/books">book lists</a>. <a href="https://news.ycombinator.com/item?id=33593631">Here’s a good thread</a> from Hacker News on books that made topics finally ‘click’. <a href="https://www.jehsmith.com/1/2020/11/the-philosophy-of-astrobiology.html">A reading list on the Philosophy of Astrobiology</a>. <a href="https://www.laphamsquarterly.org/roundtable/jorge-luis-borges-reading-list">And a Borges reading list</a>.</p>

<p>The excellent Nabeel wanted to learn more about SARS-CoV-2, so <a href="https://github.com/nqureshi/sars-cov-2/blob/master/SARS-Cov-2.ipynb">he decoded the genome</a>. And Nabeel with <a href="https://nabeelqu.co/advice">some productivity advice</a>.</p>

<p><a href="http://bookofhook.blogspot.com/2013/03/smart-guy-productivity-pitfalls.html">Some more decent productivity advice,</a> most of which cashes out to ‘JFDI’. <em>“I’ll finish it up tomorrow”</em>, I mutter to myself, as I gaze at the sticky, bulbous pile of washing up.</p>

<p><a href="https://www.nature.com/articles/s41562-016-0015">Very good paper</a> on the social sciences, science funding, and research taste.</p>

<p><a href="https://nymag.com/intelligencer/2022/11/election-results-who-won-the-midterms-david-shor.html">David Shor on the 2022 midterms.</a></p>

<p><a href="https://www.cuimc.columbia.edu/news/decades-air-pollution-undermine-immune-system">More evidence on pollution.</a></p>

<p>Tom Forth <a href="https://twitter.com/thomasforth/status/1595000478311612417">raises an excellent question</a>: why do we in the UK chose, so often, in so many ways, to be less productive? (One answer is, <a href="https://samdumitriu.substack.com/p/why-britain-struggles-to-build-infrastructure">of course</a>, NIMBYism, but the question is deeper and the answer more cultural than that.)</p>

<p><a href="https://twitter.com/matthen2/status/1596821727878991873">Smart little visualisation of a two-layer neural network learning to classify.</a></p>

<p>Working my way through <a href="https://twitter.com/RichardALJones/status/1597174222454857729">this list of excellent posts</a> on UK science and R&amp;D.</p>

<p><a href="https://wikenigma.org.uk/">Wikienigma</a>: a Wikipedia for things we don’t yet know.</p>

<p>Benjamin Reinhardt on <a href="https://progressforum.org/posts/bjnXEK4Cs2A3Cpas7/bottling-lightning">management styles in research</a>.</p>

<p><a href="https://regressstudies.substack.com/p/i-try-to-read-20-master-and-commander">A valiant effort</a>, and worth it for the (very funny) write-up.</p>

<p><a href="https://www.benkuhn.net/overconfidence/">Ben Kuhn on overconfidence.</a></p>

            </div>
            <footer class="post-meta">
              <span>10:50am. December  1, 2022.</span>
              
            </footer>
          </article>
          <hr />
        
          <article class="post text">
            <h2 class="post-title"><a href="https://jamieonsoftware.com/2022/10/09/alphatensor-scaling.html">AlphaTensor, Taste, and the Scalability of AIs</a></h2>
            <div class="post-content">
              <p>This week’s <a href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor">AlphaTensor</a> announcement shows not only that AI is getting really good at doing things, but also that DeepMind are really good at getting AI to do things.</p>

<p>Their approach is a variant of the deep reinforcement-learning-guided Monte Carlo tree search that they have applied so successfully to playing Chess and Go. What they have done, very effectively, is to design a game with the objective of finding the most efficient tensor multiplication algorithm for a matrix of some dimension.</p>

<p>There are a number of important object-level takeaways from the paper:</p>

<ul>
  <li>The search space for various algorithms and mathematical techniques is enormous</li>
  <li>Existing algorithms and techniques are often not provably the most globally efficient; there is room to improve</li>
  <li>We can improve an algorithm by paying attention to its implementation, as well as its theoretical virtues</li>
</ul>

<p>But it also teaches us some things about the meta-level, and the benefits of DeepMind’s distinctive approach:</p>

<ul>
  <li>DeepMind’s approach is scaleable vertically: the more time and compute they spend on a task, the more they can achieve.</li>
  <li>DeepMind’s approach is scaleable horizontally: the more tasks they apply their approach to, the more tasks they can achieve.</li>
</ul>

<p>I think these points suggest something important about the <a href="https://www.gwern.net/Scaling-hypothesis#scaling-hypothesis">scaling hypothesis</a>.</p>

<p>A weak version of the scaling hypothesis claims that most of what we need to improve AI performance is more compute. All we need to do is scale up the computational and data resources of a sufficiently scalable model, and we’ll get much better performance. Give the right model architecture enough data, and enough compute/time, and our problem is solvable (up to the phyical limits, or something like that.)</p>

<p>Before I begin my argument, I’ll make a presupposition: we won’t get to AGI. My hunch is that AGI is a bit of a chimera, and that progress in AI will consist mostly in the discovery and application of specialist models for narrow-domain problem solving. Some models may generalise to some extent, some may not. But over time the progress of narrow-domain AIs will far outstrip the progress in broad-domain AIs, and the market will reallocate resources accordingly.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p>Given this presupposition, we can ask an important question: does the weak scaling hypothesis hold?</p>

<p>In a world in which AI is applied to solving narrow-domain issues, AI researchers have two major tasks:</p>

<ol>
  <li>How should they decide which tasks to work on?</li>
  <li>How should they encode a given problem such that it is tractable for AI to solve?</li>
</ol>

<p>(1) is hard to decide <em>a priori</em>, because it will depend on the path-dependence progress of science (and the supporting AI). Utility-maximising functions might be helpful here: what task has the lowest cost and the highest utility? But a lot will be baked into ‘cost’ and ‘utility’, and the various cost bases of tasks will change depending on how the technology evolves, what data is available, etc.</p>

<p>Instead, AI researchers will develop heuristics, industry and academic partnerships, new data sources, etc., and that will help narrow down the range of tasks susceptible to quick resolution. In short, AI researchers will rely on <em>taste</em> and experience to guide their decisions over the medium- and long-term.</p>

<p>(2) is more analysable from first principles. Tasks need to be encoded in such a way as to make them solvable by the model: the model architecture needs to be designed in such a way as to not preclude the task; the input needs to be readable by the model; the set of training data needs to be large and varied enough to prevent overfit, and contain sufficient signal to allow generalisation to inputs outside that dataset.</p>

<p>But most importantly, the game needs to be designed <em>such that it’s the sort of game that the model can play</em>. This is what DeepMind seem especially good at: they are able to express, e.g., protein folding, or tensor decomposition, in the form of a game that their RL agent can play.</p>

<p>This ability – being able to reorganise a question in the form of a model-appropriate game – doesn’t look nearly as susceptible to Moore’s Law-style exponential speed-ups. Researchers’ insights and abilities – in other terms, researcher productivity – don’t scale exponentially.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> It takes time and energy and the serendipitousness of a well-functioning research lab to cultivate them. Scaling compute down to an effective cost of zero doesn’t help if we’re not using these models to attack the right issues (1) in the right way (2). The marginal next unit of compute will exhibit diminishing returns.</p>

<p>But if that’s the case, then it’s clear that <em>compute isn’t the bottleneck</em>. The bottleneck, instead, will be humans figuring out which problems to tackle (1) and how to regiment them, encode them, and design games/reward systems to get AI to solve them (2).</p>

<p>DeepMind have the absolute best people on the planet working on this stuff, and it takes them time and huge amounts of resources. So we end up back at <em>taste</em>. Researchers need time and experience to develop taste, and the ability to apply heuristics and deep domain knowledge – something much more ineffable and difficult to scale.</p>

<p>This doesn’t of course mean that there won’t be wonderful progress in many, many domains, brought about by ‘merely’ scaling the compute available. Even Kuhn is at pains to stress that ‘normal science’ is progress! But it does mean that the scaling hypothesis doesn’t necessarily scale research output at the same rate, at least not without the accompanying good taste.</p>

<p><strong>Thanks</strong> to Camin McCluskey and Jessica Cooper for discussion and notes.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>This is just an assumption, and one that changes the following argument significantly if it is false. A world with AGI looks, I think we can all assume, quite a bit different. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>For some empirical data on this point, <a href="https://www.nature.com/articles/467912a">see here</a>, which suggests that doubling the population of a city increases its productivity per capita by ~15%. In other words, productivity per capita scales linearly, not exponentially. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

            </div>
            <footer class="post-meta">
              <span> 8:25pm. October  9, 2022.</span>
              
            </footer>
          </article>
          
        

        
      </section>
    </div>

    <footer>
  <p>
    <strong>colophon:</strong>
    est. 2009, reset 2019. published in London.</p>
</footer>
  </body>
</html>
